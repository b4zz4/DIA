{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd6ef73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subrtade/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from analogy_creator import AnalogyCreator\n",
    "import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69519f0c",
   "metadata": {},
   "source": [
    "# Presets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44029400",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = './dataset/data/'\n",
    "out_subfolder = 'notebook_analogies'\n",
    "experiment_root = './results'\n",
    "visualize_tokens = True\n",
    "# guidance scales\n",
    "scales = [1.,2., 3., 5., 7., 9., 12.]\n",
    "# analogy strength step\n",
    "steps = np.linspace(0, 3, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65fc40a3",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5faf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Loading model from stable-diffusion/models/ldm/stable-diffusion-v1/model.ckpt\n",
      "Global Step: 194366\n",
      "LatentDiffusion: Running in eps-prediction mode\n"
     ]
    }
   ],
   "source": [
    "config = OmegaConf.load('./config/parameter_estimation.yaml')\n",
    "\n",
    "\n",
    "print('Loading model...')\n",
    "model = utils.prepare_default_model()\n",
    "print('Model loaded')\n",
    "\n",
    "\n",
    "export_path = os.path.join(experiment_root, 'analogy_results', out_subfolder)\n",
    "os.makedirs(export_path, exist_ok = True)\n",
    "\n",
    "token_subfolder = 'tokens_dia_test'\n",
    "subfolder = 'noise_dia_test'\n",
    "\n",
    "\n",
    "ddim_sampler = DDIMSampler(model)\n",
    "\n",
    "analogy_creator =  AnalogyCreator(config, ddim_sampler, subfolder, token_subfolder, os.path.join(log_config.experiment_root, 'analogy_results', out_subfolder))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4a7a064",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7738b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyplot\n",
    "\n",
    "im_path = lambda x: os.path.join(path_to_data, x)\n",
    "\n",
    "\n",
    "all_images = sorted(utils.load_all_image_names())\n",
    "labels = [utils.extract_file_id_from_path(x) for x in all_images] \n",
    "\n",
    "\n",
    "images_list = [im_path(x) for x in all_images]\n",
    "ipyplot.plot_images(images_list, labels = labels, max_images=200, img_width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627667a",
   "metadata": {},
   "source": [
    "### Triplet Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1035cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id_A = '000203'\n",
    "file_id_A_prime = '000204'\n",
    "file_id_B = '000226'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40cee5",
   "metadata": {},
   "source": [
    "# Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e0b3f",
   "metadata": {},
   "source": [
    "### Check for inverted CLIP features and noise\n",
    "The parameters will be estimated, if missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check for CLIP features\n",
    "utils.check_and_run_inversion(model, file_id_A, token_subfolder, config, tokens=True)\n",
    "\n",
    "utils.check_and_run_inversion(model,file_id_A_prime, token_subfolder, config, tokens=True)\n",
    "\n",
    "utils.check_and_run_inversion(model,file_id_B, token_subfolder, config, tokens=True)\n",
    "\n",
    "\n",
    "# check for noise of image B\n",
    "config.token_subfolder = token_subfolder\n",
    "utils.check_and_run_inversion(model, file_id_B, subfolder, config, tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49d5fb",
   "metadata": {},
   "source": [
    "### Performing analogies\n",
    "Generating grid of analogies for given triplet, list of guidance scales \\sigma_i and analogy strengths \\lambda_j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca17cd4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(f'Processing triplet: ({file_id_A}, {file_id_A_prime}, {file_id_B})')\n",
    "\n",
    "triplet = (utils.file_id2im_path(file_id_A), utils.file_id2im_path(file_id_A_prime), utils.file_id2im_path(file_id_B))\n",
    "triplet_code = f'{file_id_A}_{file_id_A_prime}_{file_id_B}'\n",
    "\n",
    "# cA = model.get_learned_conditioning('manual prompt for A')\n",
    "# cAprime = model.get_learned_conditioning(\"manual prompt for A'\")\n",
    "\n",
    "cA = analogy_creator.fetch_cond_matrix(file_id_A)\n",
    "cAprime = analogy_creator.fetch_cond_matrix(file_id_A_prime)\n",
    "\n",
    "noiseB,_,_ = utils.load_inversion_result_dict(file_id_B, subfolder, return_result_dict=False)\n",
    "\n",
    "cB = analogy_creator.fetch_cond_matrix(file_id_B)\n",
    "\n",
    "\n",
    "analogy_creator.make_analogy_from_args(triplet_code, cA, cAprime, cB, noiseB, steps, scales)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec85b4",
   "metadata": {},
   "source": [
    "### Analogy Results\n",
    "Generated grid of analogies is shown. Each row corresponds to the guidance scale \\sigma_i, each column corresponds to analogy strength \\lambda_j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=os.path.join(export_path,\n",
    "                        'grids',\n",
    "                        f'{triplet_code}_analogy_grid.jpg' )) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a069f",
   "metadata": {},
   "source": [
    "#### Visualize particular result for given \\sigma_i and \\lambda_j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c51aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 3 # [0-len(scales)]\n",
    "col = 10 # [0-len(steps)]\n",
    "current_scale = scales[row]\n",
    "current_step = steps[col]\n",
    "print(f'scale: {current_scale} | step: {current_step}')\n",
    "Image(filename=os.path.join(export_path,\n",
    "                            triplet_code,\n",
    "                        f'analogy_sc={current_scale}_shift_strength={current_step}.jpg' )) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc55236",
   "metadata": {},
   "source": [
    "### Token visualization\n",
    "Estimated CLIP features are paired with random noise images (and transformed via the reverse diffusion process) to visualize the captured concepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea2cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if visualize_tokens:\n",
    "    from torchvision.utils import save_image\n",
    "    os.makedirs(os.path.join(experiment_root,'token_visualization'),exist_ok=True)\n",
    "    def gen_random_samples(cond, file_id):\n",
    "        tokens_,_ = analogy_creator.ddim_sampler.sample(\n",
    "                        analogy_creator.config.ddim_steps,\n",
    "                        8,\n",
    "                        analogy_creator.config.shape,\n",
    "                        conditioning = cond.expand(8,-1,-1),\n",
    "                        eta=analogy_creator.config.ddim_eta,\n",
    "                        unconditional_guidance_scale=sc,\n",
    "                        unconditional_conditioning=analogy_creator.uc.expand(8,-1,-1),\n",
    "                    )\n",
    "        utils.save_latent_as_image(\n",
    "            analogy_creator.ddim_sampler.model, \n",
    "            tokens_,\n",
    "            os.path.join(experiment_root,'token_visualization',f'{file_id}.jpg')\n",
    "        )\n",
    "\n",
    "    from IPython.display import Image\n",
    "    gen_random_samples(cA, file_id_A)\n",
    "    Image(filename=os.path.join(experiment_root,'token_visualization',f'{file_id_A}.jpg')) \n",
    "    gen_random_samples(cAprime, file_id_A_prime)\n",
    "    Image(filename=os.path.join(experiment_root,'token_visualization',f'{file_id_A_prime}.jpg')) \n",
    "    gen_random_samples(cB, file_id_B)\n",
    "    Image(filename=os.path.join(experiment_root,'token_visualization',f'{file_id_B}.jpg')) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analogies",
   "language": "python",
   "name": "analogies"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
